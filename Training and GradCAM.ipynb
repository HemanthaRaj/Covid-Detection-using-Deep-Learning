{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e03a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54fb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_addition(base_img, mask):\n",
    "    feature_imp_map = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    feature_imp_map = np.float32(feature_imp_map) / 255\n",
    "    cam = feature_imp_map + np.float32(base_img)\n",
    "    cam = cam / np.max(cam)\n",
    "    super_imposed = np.uint8(255 * cam)\n",
    "    return super_imposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_data_generator(image, vgg_map, resnet_map, densenet_map, c):\n",
    "    image = image.cpu().numpy()\n",
    "    image = np.squeeze(np.transpose(image[0], (1, 2, 0)))\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + \\\n",
    "        np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    grad_models = {\n",
    "        'X-Ray Image': image,\n",
    "        'VGG-16': mask_addition(image, vgg_map),\n",
    "        'ResNet-18': mask_addition(image, resnet_map),\n",
    "        'DenseNet-121': mask_addition(image, densenet_map)\n",
    "    }\n",
    "\n",
    "    plt.style.use('seaborn-notebook')\n",
    "    fig = plt.figure(figsize=(20, 4))\n",
    "    for i, (name, img) in enumerate(grad_models.items()):\n",
    "        ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
    "        if i:\n",
    "            img = img[:, :, ::-1]\n",
    "        ax.imshow(img)\n",
    "        ax.set_xlabel(name, fontweight='bold')\n",
    "\n",
    "    fig.suptitle(\n",
    "        'grad_cl_ac_map Comparison',\n",
    "        fontweight='bold', fontsize=18\n",
    "    )\n",
    "    save_path = c + 'grad_cam.png'\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('grad_cam/' + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a59048",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dirs = {\n",
    "    'train': 'C:/Users/Hemanth/Desktop/Pre-trained with GradCAM/train',\n",
    "    'val': 'C:/Users/Hemanth/Desktop/Pre-trained with GradCAM/val',\n",
    "    'test': 'C:/Users/Hemanth/Desktop/Pre-trained with GradCAM/test'\n",
    "}\n",
    "###\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb0ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_funtion(id):\n",
    "    seed_current = torch.initial_seed()\n",
    "    seed_base = seed_current - id\n",
    "    seed_seq = np.random.SeedSequence([id, seed_base])\n",
    "    np.random.seed(seed_seq.generate_state(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36080e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tot_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "\n",
    "def tot_preds(model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_preds = torch.tensor([], device=device)\n",
    "        for batch in loader:\n",
    "            images = batch[0].to(device)\n",
    "            preds = model(images)\n",
    "            final_preds = torch.cat((final_preds, preds), dim=0)\n",
    "\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd799daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(epochs, model, criterion, optimizer, train_dl, valid_dl):\n",
    "    model_name = type(model).__name__.lower()\n",
    "    val_loss_min = np.Inf\n",
    "    train_img_count = 6416\n",
    "    val_img_count = 600\n",
    "    fields = [\n",
    "        'epoch', 'running_loss', 'running_acc', 'val_loss', 'val_acc'\n",
    "    ]\n",
    "    rows = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss =  0\n",
    "        corr_pred_train = 0\n",
    "        train_loop = tqdm(train_dl)\n",
    "\n",
    "        model.train()\n",
    "        for batch in train_loop:\n",
    "            images, labels = batch[0].to(device), batch[1].to(device)\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            corr_pred_train += tot_correct(preds, labels)\n",
    "\n",
    "            train_loop.set_description(f'Epoch [{epoch+1:2d}/{epochs}]')\n",
    "            train_loop.set_postfix(\n",
    "                loss=loss.item(), acc=corr_pred_train/train_img_count\n",
    "            )\n",
    "        running_loss = running_loss/train_img_count\n",
    "        running_acc = corr_pred_train/train_img_count\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            corr_pred_val = 0\n",
    "            for batch in valid_dl:\n",
    "                images, labels = batch[0].to(device), batch[1].to(device)\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, labels)\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "                corr_pred_val += tot_correct(preds, labels)\n",
    "\n",
    "            val_loss = val_loss/val_img_count\n",
    "            val_acc = corr_pred_val/val_img_count\n",
    "\n",
    "            rows.append([epoch, running_loss, running_acc, val_loss, val_acc])\n",
    "\n",
    "            train_loop.write(\n",
    "                f'\\nTrain loss in this Epoch: {running_loss:.6f}', end='\\t')\n",
    "            train_loop.write(f'Validation loss this Epoch: {val_loss:.6f}\\n')\n",
    "\n",
    "\n",
    "            if val_loss <= val_loss_min:\n",
    "                train_loop.write('\\t\\tSaving best model\\n')\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    f'{model_name}.pth'\n",
    "                )\n",
    "                val_loss_min = val_loss\n",
    "\n",
    "    with open(f'{model_name}.csv', 'w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(fields)\n",
    "        csv_writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd436bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_loader(pretrained=False, out_features=None, path=None):\n",
    "    model = torchvision.models.vgg16(pretrained=pretrained)\n",
    "    if out_features is not None:\n",
    "        model.classifier = torch.nn.Sequential(\n",
    "            *list(model.classifier.children())[:-1],\n",
    "            torch.nn.Linear(in_features=4096, out_features=out_features)\n",
    "        )\n",
    "    if path is not None:\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def resnet_loader(pretrained=False, out_features=None, path=None):\n",
    "    model = torchvision.models.resnet18(pretrained=pretrained)\n",
    "    if out_features is not None:\n",
    "        model.fc = torch.nn.Linear(in_features=512, out_features=out_features)\n",
    "    if path is not None:\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def densenet_loader(pretrained=False, out_features=None, path=None):\n",
    "    model = torchvision.models.densenet121(pretrained=pretrained)\n",
    "    if out_features is not None:\n",
    "        model.classifier = torch.nn.Linear(\n",
    "            in_features=1024, out_features=out_features\n",
    "        )\n",
    "    if path is not None:\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1fd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.ImageFolder(root=dirs['train'], transform=transform['train'])\n",
    "val_set = datasets.ImageFolder(root=dirs['val'], transform=transform['val'])\n",
    "\n",
    "class_tot_count = torch.as_tensor(dataset_train.targets).bincount()\n",
    "weight = 1 / class_tot_count\n",
    "samples_weight = weight[dataset_train.targets]\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset_train, batch_size=40, sampler=sampler, num_workers=0, worker_init_fn=worker_funtion)\n",
    "valid_dl = DataLoader(val_set, batch_size=40)\n",
    "\n",
    "epochs = 25\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc3ce3",
   "metadata": {},
   "source": [
    "# To Train Remove Comments. Approx training time 6 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa494896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 = vgg_loader(pretrained=True, out_features=2)\n",
    "# fit_model(\n",
    "#     epochs=epochs,\n",
    "#     model=vgg16,\n",
    "#     criterion=criterion,\n",
    "#     optimizer=optim.Adam(vgg16.parameters(), lr=3e-5),\n",
    "#     train_dl=train_dl,\n",
    "#     valid_dl=valid_dl\n",
    "# )\n",
    "\n",
    "# resnet18 = resnet_loader(pretrained=True, out_features=2)\n",
    "# fit_model(\n",
    "#     epochs=epochs,\n",
    "#     model=resnet18,\n",
    "#     criterion=criterion,\n",
    "#     optimizer=optim.Adam(resnet18.parameters(), lr=3e-5),\n",
    "#     train_dl=train_dl,\n",
    "#     valid_dl=valid_dl\n",
    "# )\n",
    "\n",
    "# densenet121 = densenet_loader(pretrained=True, out_features=2)\n",
    "# fit_model(\n",
    "#     epochs=epochs,\n",
    "#     model=densenet121,\n",
    "#     criterion=criterion,\n",
    "#     optimizer=optim.Adam(densenet121.parameters(), lr=3e-5),\n",
    "#     train_dl=train_dl,\n",
    "#     valid_dl=valid_dl\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = resnet_loader(out_features=2, path='resnet.pth')\n",
    "vgg16 = vgg_loader(out_features=2, path='vgg.pth')\n",
    "densenet121 = densenet_loader(out_features=2, path='densenet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.ImageFolder(root=dirs['train'], transform=transform['test'])\n",
    "dataset_test = datasets.ImageFolder(root=dirs['test'], transform=transform['test'])\n",
    "train_dl = DataLoader(dataset_train, batch_size=128)\n",
    "test_dl = DataLoader(dataset_test, batch_size=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f966dff",
   "metadata": {},
   "source": [
    "# Generates GradCAM for all Test Images. Could be used as a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a364e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grad_cl_ac_map:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model.eval()\n",
    "        self.fmaps = []\n",
    "        self.grads = []\n",
    "\n",
    "        target_layer.register_forward_hook(self.fl_fmap)\n",
    "        target_layer.register_backward_hook(self.grad_vals)\n",
    "\n",
    "    def fl_fmap(self, module, input, output):\n",
    "        self.fmaps.append(output)\n",
    "\n",
    "    def grad_vals(self, module, grad_input, grad_output):\n",
    "        self.grads.append(grad_output[0])\n",
    "\n",
    "    def get_cam_weights(self, grads):\n",
    "        return np.mean(grads, axis=(1, 2))\n",
    "\n",
    "    def __call__(self, image, label=None):\n",
    "        preds = self.model(image)\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        if label is None:\n",
    "            label = preds.argmax(dim=1).item()\n",
    "\n",
    "        preds[:, label].backward()\n",
    "\n",
    "        fmaps = self.fmaps[-1].cpu().data.numpy()[0, :]\n",
    "        grads = self.grads[-1].cpu().data.numpy()[0, :]\n",
    "\n",
    "        cam_weights = self.get_cam_weights(grads)\n",
    "        cam = np.zeros(fmaps.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(cam_weights):\n",
    "            cam += w * fmaps[i]\n",
    "\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, image.shape[-2:][::-1])\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "for i in range(0,600):\n",
    "    image, label = dataset_test[i]\n",
    "    image = image.unsqueeze(dim=0).to(device)\n",
    "\n",
    "    gcam = grad_cl_ac_map(model=vgg16, target_layer=vgg16.features[-1])\n",
    "    vgg_map = gcam(image, label)\n",
    "\n",
    "    gcam = grad_cl_ac_map(model=resnet18, target_layer=resnet18.layer4[-1])\n",
    "    resnet_map = gcam(image, label)\n",
    "\n",
    "    gcam = grad_cl_ac_map(model=densenet121, target_layer=densenet121.features[-1])\n",
    "    densenet_map = gcam(image, label)\n",
    "\n",
    "\n",
    "    if label == 0:\n",
    "        c = \"COVID\"\n",
    "    else:\n",
    "        c = \"Normal\"\n",
    "\n",
    "    c = c + \"_\" + str(i) + \"_\"\n",
    "\n",
    "    cam_data_generator(image, vgg_map, resnet_map, densenet_map, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
